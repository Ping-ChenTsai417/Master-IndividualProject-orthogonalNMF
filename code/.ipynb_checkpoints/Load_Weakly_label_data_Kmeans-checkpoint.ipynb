{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import glob\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt \n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "\n",
    "mypath = 'C:/Users/Tsaip/OneDrive - Imperial College London/Ovation Data Internship 2020/Data/ContenSimilarityTest/images_Landmass.mat'\n",
    "# mypath = 'C:/Users/Tsaip/OneDrive - Imperial College London/Ovation Data Internship 2020/Data/ContenSimilarityTest/images_Landmass.mat'\n",
    "\n",
    "\n",
    "# Test\n",
    "# dirs = os.listdir( mypath )\n",
    "# This would print all the files and directories\n",
    "# for file in dirs:\n",
    "#     print file name\n",
    "#     print(file)\n",
    "#     print file path\n",
    "#     print(join(mypath, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_data = scipy.io.loadmat(mypath)['images']\n",
    "# print(len(list_data))\n",
    "array_data = np.array(list_data)\n",
    "print('Shape of data array: ', array_data.shape)\n",
    "\n",
    "fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "    ax.imshow(array_data[n], cmap='afmhot')\n",
    "    ax.set_title(\"%i. \" % (n+1) + \"Class 1 Chaotic \" , fontsize=13)\n",
    "plt.show()\n",
    "fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "    ax.imshow(array_data[n+500], cmap='afmhot')\n",
    "    ax.set_title(\"%i. \" % (n+1) + \"Class 2 Unknown\", fontsize=13)\n",
    "plt.show()\n",
    "fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "    ax.imshow(array_data[n+1000], cmap='afmhot')\n",
    "    ax.set_title(\"%i. \" % (n+1) + \"Class 3 Fault\", fontsize=13)\n",
    "plt.show()\n",
    "fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "    ax.imshow(array_data[n+1500], cmap='afmhot')\n",
    "    ax.set_title(\"%i. \" % (n+1) + \"Class 4 Salt\", fontsize=13)\n",
    "plt.show()\n",
    "print('There are', len(array_data), 'texture images. Each image has dimension: ', array_data[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt  \n",
    "from matplotlib import style \n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumOfClasses = 4\n",
    "# M is total num of images\n",
    "numImages = array_data.shape[0]\n",
    "print(numImages)\n",
    "# Same number of images per class\n",
    "numImagesPerClass = int(numImages/NumOfClasses) \n",
    "print(type(numImagesPerClass))\n",
    "X = np.reshape(array_data,(numImages,array_data.shape[1]*array_data.shape[2])).T\n",
    "print('Data matrix X shape: ', X.shape)\n",
    "# Check if there is negative element in the matrix. The dataset should be normalised to [0, 1]\n",
    "if (X < 0).any():\n",
    "    print('Matrix elements are not all positive!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reference: https://www.geeksforgeeks.org/ml-determine-the-optimal-value-of-k-in-k-means-clustering/\n",
    "#  choose the optimum value of ‘k’ using elbow method\n",
    "# split features based on classes\n",
    "# =====================================================================\n",
    "X_ch = X[:,:numImagesPerClass] #Chaotic\n",
    "X_ot = X[:, numImagesPerClass:numImagesPerClass*2] # Unknown\n",
    "X_fa = X[:, numImagesPerClass*2:numImagesPerClass*3] # Fault\n",
    "X_sa = X[:, numImagesPerClass*3:numImagesPerClass*4] #Salt Dome\n",
    "# Perform kmeans on each dataset features\n",
    "\n",
    "cost_fa =[]\n",
    "cost_sa =[]\n",
    "\n",
    "# Ideally, the specified number of clusters should not exceed the number of unique data points\n",
    "# ConvergenceWarning: Number of distinct clusters (472) found smaller than n_clusters (480)\n",
    "# Possibly due to duplicate points in X.\n",
    "\n",
    "# for i in range(150, 460 ,50): \n",
    "#     KM_fa = KMeans(n_clusters = i, max_iter = 1000).fit(X_fa.T) \n",
    "#     KM_sa = KMeans(n_clusters = i, max_iter = 1000).fit(X_sa.T) \n",
    "#     # calculates squared error \n",
    "#     # for the clustered points \n",
    "#     cost_fa.append(KM_fa.inertia_)\n",
    "#     cost_sa.append(KM_sa.inertia_)\n",
    "\n",
    "print(array_data[2][1,0])\n",
    "X_ch[99,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cost_fa and cost_sa to see elbow method result.\n",
    "# ======================================================\n",
    "# import pickle\n",
    "# with open('cost_fa', 'wb') as fp:\n",
    "#     pickle.dump(cost_fa, fp)\n",
    "# with open('cost_sa', 'wb') as fp:\n",
    "#     pickle.dump(cost_sa, fp)\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read cost_fa and cost_sa it back:\n",
    "# ======================================================\n",
    "# with open('cost_fa', 'wb') as fp:\n",
    "#     cost_fa = pickle.load(fp)\n",
    "# with open('cost_sa', 'wb') as fp:\n",
    "#     cost_sa = pickle.load(fp)\n",
    "# =======================================================\n",
    "# plot the cost against K values \n",
    "# =======================================================\n",
    "# f, axarr = plt.subplots(1,2, figsize=(10, 5))\n",
    "# axarr[0].plot(range(150, 460 ,50), cost_fa, color ='r', linewidth ='3') \n",
    "# axarr[1].plot(range(150, 460 ,50), cost_sa, color ='b', linewidth ='3') \n",
    "\n",
    "# axarr[0].set_title(\"Dataset = Fault. Iteration = 1k\", fontsize=13)\n",
    "# axarr[1].set_title(\"Dataset = Salt dome. Iteration = 1k\", fontsize=13)\n",
    "\n",
    "# axarr[0].set(xlabel='Value of K', ylabel='Sqaured Error (Cost)')\n",
    "# axarr[1].set(xlabel='Value of K', ylabel='Sqaured Error (Cost)')\n",
    "# =======================================================\n",
    "# the point of the elbow is the  \n",
    "# most optimal value for choosing k \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_p = len(X_ch)\n",
    "set_sparsity = 0.5\n",
    "\n",
    "L1toL2 = sqrt(N_p) - sqrt(N_p-1)*set_sparsity # L1L2ratio\n",
    "# X = np.reshape(array_data,(array_data.shape[1]*array_data.shape[2], numImages)).T\n",
    "# print(X.shape)\n",
    "# print(X[3, 4])\n",
    "# print(array_data[4][0,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Nonnegative Double Singular Value Decomposition (NNDSVD)\n",
    "            initialization (better for sparseness)\n",
    "            NNDSVD with zeros filled with small random values\n",
    "            (generally faster, less accurate alternative to NNDSVDa\n",
    "            for when sparsity is not desired)'''\n",
    "# Chaotic\n",
    "NMFmodel= NMF(n_components=250,init='nndsvdar', solver='mu', beta_loss='frobenius', tol=0.0001, max_iter=500, random_state=None, alpha=0.0, l1_ratio=L1toL2)\n",
    "W_ch = NMFmodel.fit_transform(X_ch)\n",
    "H_ch = NMFmodel.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unknown\n",
    "# NMFmodel_o= NMF(n_components=250,init='nndsvdar', solver='mu', beta_loss='frobenius', tol=0.0001, max_iter=500, random_state=None, alpha=0.0, l1_ratio=L1toL2)\n",
    "W_o = NMFmodel.fit_transform(X_ot)\n",
    "H_o = NMFmodel.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salt Dome\n",
    "# NMFmodel= NMF(n_components=250,init='nndsvdar', solver='mu', beta_loss='frobenius', tol=0.0001, max_iter=500, random_state=None, alpha=0.0, l1_ratio=L1toL2)\n",
    "W_s = NMFmodel.fit_transform(X_sa)\n",
    "H_s = NMFmodel.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fault\n",
    "# NMFmodel_s= NMF(n_components=250,init='nndsvdar', solver='mu', beta_loss='frobenius', tol=0.0001, max_iter=500, random_state=None, alpha=0.0, l1_ratio=L1toL2)\n",
    "W_f = NMFmodel.fit_transform(X_fa)\n",
    "H_f = NMFmodel.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This qpproach will make the feature reduce to 250 for the whole dataset, instead of 250 for every classes.\n",
    "# nmf= NMF(n_components=250,init='nndsvdar', solver='mu', beta_loss='frobenius', tol=0.0001, max_iter=700, random_state=None, alpha=0.0, l1_ratio=L1toL2)\n",
    "# W2 = nmf.fit_transform(X)\n",
    "# H2 = nmf.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked plot salt and fault\n",
    "nmf_ch = np.dot(W_ch,H_ch)\n",
    "nmf_o = np.dot(W_o,H_o)\n",
    "nmf_s = np.dot(W_s,H_s)\n",
    "nmf_f = np.dot(W_f,H_f)\n",
    "print(nmf_o.shape)\n",
    "# Reshape for plotting\n",
    "img_ch_nmf = np.reshape(nmf_ch.T, (numImagesPerClass,99,99))\n",
    "img_o_nmf = np.reshape(nmf_o.T, (numImagesPerClass,99,99))\n",
    "img_s_nmf = np.reshape(nmf_s.T, (numImagesPerClass,99,99))\n",
    "img_f_nmf = np.reshape(nmf_f.T, (numImagesPerClass,99,99))\n",
    "\n",
    "\n",
    "print(img_ch_nmf[7][0,0])\n",
    "print(nmf_ch[0, 7])\n",
    "\n",
    "print(img_ch_nmf.shape)\n",
    "\n",
    "# fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "# for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "#     ax.imshow(img_ch_nmf[n], cmap='afmhot')\n",
    "#     ax.set_title(\"%i. \" % (n+1) + \"Class 1 Chaotic \" , fontsize=13)\n",
    "# plt.show()\n",
    "# fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "# for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "#     ax.imshow(img_o_nmf[n], cmap='afmhot')\n",
    "#     ax.set_title(\"%i. \" % (n+1) + \"Class 2 Unknown\", fontsize=13)\n",
    "# plt.show()\n",
    "# fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "# for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "#     ax.imshow(img_f_nmf[n], cmap='afmhot')\n",
    "#     ax.set_title(\"%i. \" % (n+1) + \"Class 3 Fault\", fontsize=13)\n",
    "# plt.show()\n",
    "# fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "# for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "#     ax.imshow(img_s_nmf[n], cmap='afmhot')\n",
    "#     ax.set_title(\"%i. \" % (n+1) + \"Class 4 Salt\", fontsize=13)\n",
    "# plt.show()\n",
    "# print('There are', 4*len(img_s_nmf), 'texture images. Each image has dimension: ', img_s_nmf[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate W and H to to facilitate stacked calculation\n",
    "k = 250\n",
    "H_vstack = np.concatenate((H_ch, H_o, H_f, H_s), axis = 0)\n",
    "H_conc = np.zeros((k*NumOfClasses, numImages))\n",
    "W_conc = np.concatenate((W_ch, W_o, W_f, W_s), axis = 1)\n",
    "H_conc[:k, :numImagesPerClass] = H_ch\n",
    "H_conc[k:k*2, numImagesPerClass:numImagesPerClass*2] = H_o\n",
    "H_conc[k*2:k*3, numImagesPerClass*2:numImagesPerClass*3] = H_f\n",
    "H_conc[k*3:k*4, numImagesPerClass*3:numImagesPerClass*4] = H_s\n",
    "\n",
    "# WH_total = np.dot(W_conc, H_conc)\n",
    "# img_nmf = np.reshape(WH_total.T, (numImages,99,99))\n",
    "\n",
    "# fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "# for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "#     ax.imshow(img_nmf[n], cmap='afmhot')\n",
    "#     ax.set_title(\"%i. \" % (n+1) + \"Class 1 Chaotic \" , fontsize=13)\n",
    "# plt.show()\n",
    "# fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "# for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "#     ax.imshow(img_nmf[n+500], cmap='afmhot')\n",
    "#     ax.set_title(\"%i. \" % (n+1) + \"Class 2 Unknown\", fontsize=13)\n",
    "# plt.show()\n",
    "# fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "# for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "#     ax.imshow(img_nmf[n+1000], cmap='afmhot')\n",
    "#     ax.set_title(\"%i. \" % (n+1) + \"Class 3 Fault\", fontsize=13)\n",
    "# plt.show()\n",
    "# fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "# for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "#     ax.imshow(img_nmf[n+1500], cmap='afmhot')\n",
    "#     ax.set_title(\"%i. \" % (n+1) + \"Class 4 Salt\", fontsize=13)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeColumns(matrix):\n",
    "# normalizes the input matrix to have the L1 norm of each column = 1\n",
    "    output = np.zeros((matrix.shape))\n",
    "    for i in range(matrix.shape[1]):\n",
    "        output[:,i] = np.divide(matrix[:,i], sum(abs(matrix[:,i])))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing\n",
    "Q = np.kron(np.eye(NumOfClasses, NumOfClasses), np.ones((250,1)))\n",
    "Hi = np.reshape(H_vstack[:,1],(len(H_vstack[:,1]),1))\n",
    "WH_i = np.dot(W_conc, np.multiply(Q, Hi))\n",
    "W_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.kron(np.eye(NumOfClasses, NumOfClasses), np.ones((250,1)))\n",
    "Q = normalizeColumns(Q)\n",
    "Y = np.zeros((numImages, 99*99, NumOfClasses))\n",
    "n = 0\n",
    "# Just for testing\n",
    "H_multi = np.concatenate((H_vstack, H_vstack, H_vstack, H_vstack), axis = 1)\n",
    "for img in range(0, numImages):\n",
    "    Hi = np.reshape(H_multi[:,img],(len(H_multi[:,img]),1))\n",
    "    n = n+1\n",
    "    if n >=500:\n",
    "        n = 0\n",
    "    \n",
    "    # map the coefficients of each image into the seismic structures that make up that image\n",
    "    # Each Y shows the likelihood of each seismic structure for each pixel in the image.\n",
    "    WH_i = np.dot(W_conc, np.multiply(Q, Hi))\n",
    "#     print('len(WH_product)',len(WH_product))\n",
    "    Y[img,:, :] = WH_i\n",
    "# oo = np.multiply(Q,Hi)\n",
    "# WH_product = np.dot(W_s, oo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y[0,0,0])\n",
    "print(Y[0,0,1])\n",
    "D = numpy.unique(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cluster membership matrix Q to \n",
    "# test = kron(eye(N_l,N_l),ones(250,500))\n",
    "Q = np.kron(1, np.ones((250,1)))\n",
    "Q = normalizeColumns(Q)\n",
    "print('Q',Q.shape)\n",
    "print(H_s.shape)\n",
    "Y = np.zeros((500,99*99))\n",
    "for img in range(0, 500):\n",
    "    Hi_ch = np.reshape(H_ch[:,img],(len(H_ch[:,img]),1))\n",
    "#     Hi_o = np.reshape(H_o[:,img],(len(H_o[:,img]),1))\n",
    "#     Hi_f = np.reshape(H_f[:,img],(len(H_f[:,img]),1))\n",
    "#     Hi_s = np.reshape(H_s[:,img],(len(H_s[:,img]),1))\n",
    "\n",
    "# map the coefficients of each image into the seismic structures that make up that image\n",
    "# Each Y shows the likelihood of each seismic structure for each pixel in the image.\n",
    "    WH_ch = np.dot(W_ch, np.multiply(Q,Hi_ch))\n",
    "#     print('len(WH_product)',len(WH_product))\n",
    "    Y[img,:] = np.reshape(WH_ch.T, (len(WH_ch),))\n",
    "# oo = np.multiply(Q,Hi)\n",
    "# WH_product = np.dot(W_s, oo)\n",
    "\n",
    "# Check nonnegativity\n",
    "zerocoeff = np.where(WH_ch<=0)\n",
    "print('There are %i negative element in WH_product.'%len(zerocoeff[0]))\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "sigma = 0.4; #sigma value for the gaussian filtering: Standard deviation for Gaussian kernel. \n",
    "# temp = Y[0,:]\n",
    "temp = gaussian_filter(Y, sigma = sigma)\n",
    "print(temp.shape)\n",
    "zerocoeff11 = np.where(temp<=0)\n",
    "print('There are %i negative element in WH_product.'%len(zerocoeff11[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(data_mat,Y,N_label,conf_thresh, gaussian_filtering, sigma):\n",
    "    return\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample images\n",
    "conf_thresh = 0; # threshold any values below this value to zero\n",
    "gaussian_filtering = 1; # set to 1 to use gaussian filtering of the results\n",
    "sigma = 0.5; #sigma value for the gaussian filtering: Standard deviation for Gaussian kernel. \n",
    "N_label = 1 # Number of label\n",
    "plot_images(X,Y,N_label,conf_thresh, gaussian_filtering, sigma)\n",
    "\n",
    "idx = 0\n",
    "imgPerClass = 6; # how many images to show for each class\n",
    "# indexes = round(linspace(1,size(X,2), N_l*imgPerClass));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the vector\n",
    "#  Do we need kron??????\n",
    "y = np.kron(np.array([1, 2, 3, 4]), np.ones((1,numImagesPerClass)))\n",
    "# Initialise W1 by applying kmeans on each class\n",
    "k_clusters = 250\n",
    "# Perform kmeans on each dataset features\n",
    "kmeans_ch = KMeans(n_clusters=k_clusters, max_iter=1000).fit(X_ch.T)\n",
    "kmeans_ot = KMeans(n_clusters=k_clusters, max_iter=1000).fit(X_ot.T)\n",
    "kmeans_fa = KMeans(n_clusters=k_clusters, max_iter=1000).fit(X_fa.T)\n",
    "kmeans_sa = KMeans(n_clusters=k_clusters, max_iter=1000).fit(X_sa.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans_ch.cluster_centers_.shape)\n",
    "# print('Chaotic clusters : \\n',kmeans_ch.cluster_centers_)\n",
    "centr_ch = kmeans_ch.cluster_centers_.T\n",
    "centr_ot = kmeans_ot.cluster_centers_.T\n",
    "centr_fa = kmeans_fa.cluster_centers_.T\n",
    "centr_sa = kmeans_sa.cluster_centers_.T\n",
    "# initialize W1_ W2_ and H_\n",
    "# W_init_list = [centr_ch.T, centr_ot.T, centr_fa.T, centr_sa.T]\n",
    "# W=M*r, H = r*N, V = W.H, W is basis matrix, each column is a basis vector\n",
    "np.random.seed(20)\n",
    "H_init = np.random.uniform(0.0, 1.0, size = (NumOfClasses*k_clusters, numImages))\n",
    "# H_init = np.random.randn(NumOfClasses*k_clusters, numImages)\n",
    "W_init = np.hstack((centr_ch, centr_ot, centr_fa, centr_sa))\n",
    "# Make sure all the centroids are nonnegative\n",
    "W_init[W_init<0.0001] = 0\n",
    "# print('H_init : \\n',H_init)\n",
    "pop = np.where(centr_ch==0)\n",
    "print(pop[0].shape)\n",
    "print(W_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code test sklearn NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_init.shape\n",
    "W_init\n",
    "W_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1_ratio=L1toL2\n",
    "modelTest1 = NMF(n_components=1000, init='custom', solver='mu', beta_loss='frobenius', tol=0.0001, max_iter=600, random_state=None, alpha=0.1, l1_ratio=L1toL2)\n",
    "# Sparsity  = 0.4\n",
    "W_all = modelTest1.fit_transform(X, W = W_init, H = H_init)\n",
    "H_all = modelTest1.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTest = NMF(n_components=1000, init='nndsvdar', solver='mu', beta_loss='frobenius', tol=0.00001, max_iter=600, random_state=None, alpha=0.18, l1_ratio=L1toL2)\n",
    "# Sparsity  = 0.4\n",
    "W_all = modelTest.fit_transform(X)\n",
    "H_all = modelTest.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine single class of H result onto all class result H\n",
    "# Concatenate W and H to to facilitate stacked calculation\n",
    "k = 250\n",
    "H_vstack = np.concatenate((H_ch, H_o, H_f, H_s), axis = 0)\n",
    "H_concat = H_all\n",
    "W_conca = np.concatenate((W_ch, W_o, W_f, W_s), axis = 1)\n",
    "H_concat[:k, :numImagesPerClass] = H_ch\n",
    "H_concat[k:k*2, numImagesPerClass:numImagesPerClass*2] = H_o\n",
    "H_concat[k*2:k*3, numImagesPerClass*2:numImagesPerClass*3] = H_f\n",
    "H_concat[k*3:k*4, numImagesPerClass*3:numImagesPerClass*4] = H_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1_singleNMFh = H_conc[0:k,0]\n",
    "img_1_singleNMFh.shape\n",
    "img_1_allNMFh = H_all[0:k,0]\n",
    "img_1_allNMFh.shape\n",
    "\n",
    "img_1_singleNMFw = W_conc[0,0:k]\n",
    "img_1_singleNMFw.shape\n",
    "img_1_allNMFw = W_all[0,0:k]\n",
    "img_1_allNMFw.shape\n",
    "# \n",
    "img_150_singleNMFw = W_conc[0,k*2:k*3]\n",
    "img_150_singleNMFw.shape\n",
    "img_150_allNMFw = W_all[0,k*2:k*3]\n",
    "img_150_allNMFw.shape\n",
    "\n",
    "img_150_singleNMFh = H_f[:,0] # H_conc[k*2:k*3,0]\n",
    "img_150_singleNMFh.shape\n",
    "img_150_allNMFh = H_all[k*2:k*3,0]\n",
    "img_150_singleNMFh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the difference between stacked W NMF output and single W NMF output\n",
    "f, axs = plt.subplots(2,1,figsize=(12,10))\n",
    "lg1, = axs[0].plot(np.arange(k), img_1_singleNMFw, 'bx', label='Single W')\n",
    "lg2, = axs[0].plot(np.arange(k), img_1_allNMFw,'r.', label = 'all W')\n",
    "axs[0].set_xlabel(\"Index of feature\")\n",
    "axs[0].set_ylabel(\"Magnitude\")\n",
    "axs[0].set_title(\"W for the 1st image\")\n",
    "axs[0].legend(handles=[lg1, lg2], loc = 'upper right')\n",
    "# ax[0].legend(loc='upper left', frameon=False)\n",
    "dt = 0.1\n",
    "axs[1].psd(img_1_singleNMFw, 512, 1 / dt, label='Single W')\n",
    "axs[1].psd(img_1_allNMFw, 512, 1 / dt,  label = 'all W')\n",
    "axs[1].set_title('PSD of W')\n",
    "axs[1].legend(['Single W','all W'], loc = 'upper right')\n",
    "# plt.psd(img_1_singleNMFw, 512, 1 / dt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the difference between stacked W NMF output and single W NMF output\n",
    "f, axs = plt.subplots(2,1,figsize=(12,10))\n",
    "lg1, = axs[0].plot(np.arange(k), img_150_singleNMFw, 'bx', label='Single W')\n",
    "lg2, = axs[0].plot(np.arange(k), img_150_allNMFw,'r.', label = 'all W')\n",
    "axs[0].set_xlabel(\"Index of feature\")\n",
    "axs[0].set_ylabel(\"Magnitude\")\n",
    "axs[0].set_title(\"W for the nst image\")\n",
    "axs[0].legend(handles=[lg1, lg2], loc = 'upper right')\n",
    "# ax[0].legend(loc='upper left', frameon=False)\n",
    "dt = 0.1\n",
    "axs[1].psd(img_150_singleNMFw, 512, 1 / dt, label='Single W')\n",
    "axs[1].psd(img_150_allNMFw, 512, 1 / dt,  label = 'all W')\n",
    "axs[1].set_title('PSD of W')\n",
    "axs[1].legend(['Single W','all W'], loc = 'upper right')\n",
    "# plt.psd(img_1_singleNMFw, 512, 1 / dt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the difference between stacked W NMF output and single W NMF output\n",
    "f, axs = plt.subplots(2,1,figsize=(12,10))\n",
    "lg1, = axs[0].plot(np.arange(k), img_1_singleNMFh, 'bx', label='Single H')\n",
    "lg2, = axs[0].plot(np.arange(k), img_1_allNMFh,'r.', label = 'all H')\n",
    "axs[0].set_xlabel(\"Index of feature\")\n",
    "axs[0].set_ylabel(\"Magnitude\")\n",
    "axs[0].set_title(\"H for the nst image\")\n",
    "axs[0].legend(handles=[lg1, lg2], loc = 'upper right')\n",
    "# ax[0].legend(loc='upper left', frameon=False)\n",
    "dt = 0.1\n",
    "axs[1].psd(img_1_singleNMFh, 512, 1 / dt,  label='Single H')\n",
    "axs[1].psd(img_1_allNMFh, 512, 1 / dt, label = 'all H')\n",
    "axs[1].set_title('PSD of H')\n",
    "axs[1].legend(['Single W','all W'], loc = 'upper right')\n",
    "# plt.psd(img_1_singleNMFw, 512, 1 / dt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the difference between stacked W NMF output and single W NMF output\n",
    "f, axs = plt.subplots(2,1,figsize=(12,10))\n",
    "lg1, = axs[0].plot(np.arange(k), img_150_singleNMFh, 'bx', label='Single H')\n",
    "lg2, = axs[0].plot(np.arange(k), img_150_allNMFh,'r.', label = 'all H')\n",
    "axs[0].set_xlabel(\"Index of feature coefficient\")\n",
    "axs[0].set_ylabel(\"Magnitude\")\n",
    "axs[0].set_title(\"H for the nst image\")\n",
    "axs[0].legend(handles=[lg1, lg2], loc = 'upper right')\n",
    "# ax[0].legend(loc='upper left', frameon=False)\n",
    "dt = 0.1\n",
    "axs[1].psd(img_150_singleNMFh, 512, 1 / dt, label='Single H')\n",
    "axs[1].psd(img_150_allNMFh, 512, 1 / dt,  label = 'all H')\n",
    "axs[1].set_title('PSD of H')\n",
    "axs[1].legend(['Single H','all H'], loc = 'upper right')\n",
    "# plt.psd(img_1_singleNMFw, 512, 1 / dt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WH_product = np.dot(W_all, H_conc)\n",
    "# WH_product = np.dot(W_all, H_all)\n",
    "img_nmf_all = np.reshape(WH_product.T, (numImages,99,99))\n",
    "\n",
    "fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "    ax.imshow(img_nmf_all[n], cmap='afmhot')\n",
    "    ax.set_title(\"%i. \" % (n+1) + \"Class 1 Chaotic \" , fontsize=13)\n",
    "plt.show()\n",
    "fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "    ax.imshow(img_nmf_all[n+500], cmap='afmhot')\n",
    "    ax.set_title(\"%i. \" % (n+1) + \"Class 2 Unknown\", fontsize=13)\n",
    "plt.show()\n",
    "fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "    ax.imshow(img_nmf_all[n+1000], cmap='afmhot')\n",
    "    ax.set_title(\"%i. \" % (n+1) + \"Class 3 Fault\", fontsize=13)\n",
    "plt.show()\n",
    "fig, axarr = plt.subplots(1, 5, figsize=(11, 11))\n",
    "for ax, n in zip(axarr.flatten(), np.arange(5)):\n",
    "    ax.imshow(img_nmf_all[n+1500], cmap='afmhot')\n",
    "    ax.set_title(\"%i. \" % (n+1) + \"Class 4 Salt\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qr = np.kron(np.eye(NumOfClasses, NumOfClasses), np.ones((250,1)))\n",
    "Qr = normalizeColumns(Qr)\n",
    "Y = np.zeros((numImages, 99*99, NumOfClasses))\n",
    "N_class = np.ones((1,NumOfClasses))\n",
    "\n",
    "for img in range(0, numImages):\n",
    "    Hi = np.reshape(H_conc[:,img],(len(H_conc[:,img]),1))\n",
    "    # map the coefficients of each image into the seismic structures that make up that image\n",
    "    # Each Y shows the likelihood of each seismic structure for each pixel in the image.\n",
    "    H_class = np.dot(Hi,N_class)\n",
    "    WH_i = np.dot(W_conc, np.multiply(Qr, H_class))\n",
    "#     print('len(WH_product)',len(WH_product))\n",
    "    Y[img,:, :] = WH_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "# sigma = 0.4; #sigma value for the gaussian filtering: Standard deviation for Gaussian kernel. \n",
    "\n",
    "# # temp = Y[0,:]\n",
    "# temp = gaussian_filter(Y, sigma = sigma)\n",
    "# print(temp.shape)\n",
    "# zerocoeff11 = np.where(temp<=0)\n",
    "# print('There are %i negative element in filtered data.'%len(zerocoeff11[0]))\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "gaussian_filtering = True\n",
    "sigma = 0.4\n",
    "if gaussian_filtering:\n",
    "    for label in range(NumOfClasses):\n",
    "        temp = np.reshape(Y[1, : ,label],(99,99))\n",
    "        temp = gaussian_filter(temp,sigma)\n",
    "        Y[1,:,label] = np.reshape(temp,(99*99,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals= np.max(Y[1,:,:],axis = 1)\n",
    "vals.shape\n",
    "classImg = np.argmax(Y[1,:,:],axis = 1)\n",
    "# Check how many pixels are label 2\n",
    "print(classImg.shape)\n",
    "label_ = np.where(classImg==0)\n",
    "print(len(label_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code is writing NMF from scratch. Have finished writing sparse constraint application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_sparsity = 0.4\n",
    "# Create an dentity matrix for  orthogonality constraint\n",
    "B_mat  = np.identity(k_clusters * NumOfClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsify_columns.m 正式\n",
    "mat = W_init \n",
    "\n",
    "#     Number of pixel\n",
    "N_p = len(mat)\n",
    "#  desired L1 to L2 ratio to acheive sparsity level:\n",
    "L1toL2 = sqrt(N_p) - sqrt(N_p-1)*set_sparsity # L1L2ratio\n",
    "#L2 norms of columns of matrix\n",
    "L2W_norm = np.linalg.norm(mat, axis = 0) #L2W\n",
    "#     Apply sparseness constraints on W_init\n",
    "for i in range(0, mat.shape[1]):\n",
    "    col = mat[:, i]\n",
    "    # set colume to achieve desired sparseness \n",
    "#     L1W_norm = L1toL2*L2W_norm[i]\n",
    "#     scol = projfunc(col,L1W_norm,L2W[i]**2,1,sparsity)\n",
    "#     update:\n",
    "#     mat[:i] = scol\n",
    "#     sparse_matrix = matrix\n",
    "mat.shape[1]\n",
    "\n",
    "# kk = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "# print(kk)\n",
    "# kk_norm = np.linalg.norm(kk, axis = 0)\n",
    "# kk_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection(s, L1, L2sqr, nonNeg, set_sparsity):\n",
    "    '''\n",
    "    Projection operator that imposes sparseness\n",
    "    by explicitly setting both L1 and L2 norms (and enforcing non-negativity).\n",
    "\n",
    "    '''\n",
    "#     nonNeg = True #non-negativity constraint flag. \n",
    "#     i = 2\n",
    "#     s = mat[:, i] # col\n",
    "#     L1 = L1toL2 * L2W_norm[i]\n",
    "#     L2sqr = L2W_norm[i]**2\n",
    "    # Number of feature\n",
    "    N = len(s)\n",
    "\n",
    "    try: \n",
    "        nonNeg\n",
    "    except NameError: \n",
    "    #     nonNeg = None\n",
    "#         print('non-negativity flag is not set')\n",
    "        isneg = s<0 # save column negativity state\n",
    "        s = abs(s) # take absolute of the colume.\n",
    "\n",
    "    # projecting the point to the sum constraint hyperplane\n",
    "    v = s + (L1-sum(s))/N\n",
    "    # Initialize an array for zero valued components\n",
    "    zerocoeff = []\n",
    "    j = 0\n",
    "    while 1:\n",
    "        midpoint = np.ones((N,1))*L1/(N-len(zerocoeff))#  projection operator by Hoyer(2004)\n",
    "        midpoint[zerocoeff] = 0\n",
    "        midpoint=np.reshape(midpoint,(len(midpoint),))\n",
    "        w = v-midpoint\n",
    "        a = sum(w**2)\n",
    "        # b = 2*w.T*v\n",
    "        b = 2*w@v\n",
    "        c = round(sum(v**2)-L2sqr, 5) \n",
    "        alphap = (-b+np.real(sqrt(b**2-4*a*c)))/(2*a)\n",
    "        v = alphap*w + v\n",
    "#         print('c: ',c) # Why is c always zero?\n",
    "        if all(vv>=0 for vv in v):\n",
    "            # Solution found\n",
    "#             print('All elements in v are non-negative')\n",
    "            itrations = j+1\n",
    "            break\n",
    "        j += 1\n",
    "\n",
    "        # Set negs to zero, subtract appropriate amount from rest\n",
    "        zerocoeff = np.where(v<=0)\n",
    "    #     print(\"Replace the negative values of the array with 0\")\n",
    "        v[v <= 0] = 0\n",
    "        tempsum = sum(v)\n",
    "        v = v + (L1-tempsum)/(N_p-len(zerocoeff)) #Calculate c := (sum(s)−L1)/(dim(x)−size(Z))\n",
    "        v[v <= 0] = 0\n",
    "        new_sparsity = sum(v==0)/len(v)\n",
    "        if new_sparsity > set_sparsity:\n",
    "    #         zerocoeff =  np.where(v<=0)\n",
    "            v[v <= 0] = 0\n",
    "            itrations = j\n",
    "#             print('WARNING: sparsity contraint function does not converges')\n",
    "            break\n",
    "    # sum(v.^2)=k2 which is closest to s in the euclidian sense\n",
    "    try: \n",
    "        nonNeg\n",
    "    except NameError: \n",
    "    #     nonNeg = None\n",
    "        print('Return v\\'s original sign')\n",
    "    #     (-2*isneg + 1) make the non-nagative element index  -1\n",
    "        v = np.multiply((-2*isneg + 1), v) # Return original signs to solution\n",
    "    if any(abs(v.imag))>1e-5:\n",
    "        print('ERROR: you have imaginary values!')\n",
    "\n",
    "    return v, itrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsify_columns.m\n",
    "mat = W_init \n",
    "\n",
    "#     Number of pixel\n",
    "N_p = len(mat)\n",
    "#  desired L1 to L2 ratio to acheive sparsity level:\n",
    "L1toL2 = sqrt(N_p) - sqrt(N_p-1)*set_sparsity # L1L2ratio\n",
    "#L2 norms of columns of matrix\n",
    "L2W_norm = np.linalg.norm(mat, axis = 0) #L2W\n",
    "#     Apply sparseness constraints on W_init\n",
    "i = 1\n",
    "# col = mat[:, i]\n",
    "# L1W_norm = L1toL2*L2W_norm[i]\n",
    "# projection(col, L1W_norm, L2W_norm[i]**2, True, set_sparsity)\n",
    "start = time.time()\n",
    "for i in range(0, mat.shape[1]):\n",
    "    col = mat[:, i]\n",
    "#     set colume to achieve desired sparseness \n",
    "    L1W_norm = L1toL2*L2W_norm[i]\n",
    "    scol, itr = projection(col, L1W_norm, L2W_norm[i]**2, True, set_sparsity)\n",
    "#     update:\n",
    "    mat[:, i] = scol\n",
    "sparse_matrix = mat\n",
    "end = time.time()\n",
    "print(\"Elapsed time for applying sparseness constraint:\", \n",
    "                                         end - start, 'seconds.')  \n",
    "\n",
    "# kk = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "# print(kk)\n",
    "# kk_norm = np.linalg.norm(kk, axis = 0)\n",
    "# kk_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = mat\n",
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_t = time.process_time()\n",
    "duration = time.process_time()\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midpoint = np.ones((N,1))*L1/(N-len(zerocoeff))#  projection operator by Hoyer(2004)\n",
    "print(v.shape)\n",
    "midpoint[zerocoeff] = 0\n",
    "midpoint=np.reshape(midpoint,(len(midpoint),))\n",
    "w = v-midpoint\n",
    "a = sum(w**2)\n",
    "# b = 2*w.T*v\n",
    "b = 2*w@v\n",
    "c = round(sum(v**2)-L2sqr, 5) # Why is c always zero?\n",
    "alphap = (-b+np.real(sqrt(b**2-4*a*c)))/(2*a);\n",
    "v = alphap*w + v\n",
    "print('c: ',c)\n",
    "if all(vv>=0 for vv in v):\n",
    "    # Solution found\n",
    "    print('All elements in v are nonnegaative')\n",
    "    usediters = j+1\n",
    "j = j+1\n",
    "        \n",
    "# Set negs to zero, subtract appropriate amount from rest\n",
    "zerocoeff = np.where(v<=0)\n",
    "print(\"Replace the negative values of the array with 0\")\n",
    "v[v <= 0] = 0\n",
    "tempsum = sum(v)\n",
    "v = v + (L1-tempsum)/(N_p-len(zerocoeff)) #Calculate c := (sum(s)−L1)/(dim(x)−size(Z))\n",
    "v[v <= 0] = 0\n",
    "\n",
    "new_sparsity = sum(v==0)/len(v)\n",
    "if new_sparsity > set_sparsity:\n",
    "#         zerocoeff =  np.where(v<=0)\n",
    "    v[v <= 0] = 0\n",
    "    print('WARNING: sparsity contraint function does not converges')\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Check for problems\n",
    "kkk= np.where(np.iscomplex(v)==True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a, b, c)\n",
    "haha = b**2-4*a*c\n",
    "# ii = sqrt(b**2-4*a*c)\n",
    "# kk=(-419.1157+np.real(sqrt( 419.1157**2-4*209.5578* (-1.4916e+03))))/(2*209.5578)\n",
    "mat.shape[1]\n",
    "\n",
    "kk = np.array([[-1,2,3],[-4,5,6],[-7,8,9]])\n",
    "print(kk)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
